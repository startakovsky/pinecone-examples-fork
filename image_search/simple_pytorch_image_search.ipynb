{
 "cells": [
  {
   "attachments": {
    "96812234.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "567644fa",
   "metadata": {},
   "source": [
    "![asdf](attachment:96812234.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-simulation",
   "metadata": {
    "id": "finished-simulation",
    "papermill": {
     "duration": 0.058948,
     "end_time": "2021-04-28T22:35:28.242456",
     "exception": false,
     "start_time": "2021-04-28T22:35:28.183508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Image Similarity Search\n",
    "\n",
    "In this tutorial notebook, we will walk you through how to create an image similarity search backend service. You will learn how to use the pre-trained embedding model called `squeezenet` from `torchvision` to transform image data into vector embeddings. You will build an index with Pinecone to store these vector embeddings. Lastly, you will learn how to send a new image as query, and retrieve similar images in the index.\n",
    "\n",
    "\n",
    "\n",
    "The structure of this notebook is as follows:\n",
    "\n",
    "1. Install dependencies and set up Pinecone.\n",
    "1. Download an image dataset (e.g., TinyImageNet).\n",
    "1. Download a pre-trained computer vision (CV) model (e.g. squeezenet).\n",
    "1. Using the CV model, convert images to their vector embeddings.\n",
    "1. Create a Pinecone vector index, which serves as the backend service.\n",
    "1. Upload and index your image vectors.\n",
    "1. Perform image similarity search and review the results.\n",
    "1. Delete the index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-louis",
   "metadata": {
    "id": "accomplished-louis",
    "papermill": {
     "duration": 0.04542,
     "end_time": "2021-04-28T22:35:28.335525",
     "exception": false,
     "start_time": "2021-04-28T22:35:28.290105",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-vacuum",
   "metadata": {
    "id": "nutritional-vacuum",
    "papermill": {
     "duration": 0.043416,
     "end_time": "2021-04-28T22:35:28.422133",
     "exception": false,
     "start_time": "2021-04-28T22:35:28.378717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "designing-dividend",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-28T22:35:28.515988Z",
     "iopub.status.busy": "2021-04-28T22:35:28.515371Z",
     "iopub.status.idle": "2021-04-28T22:37:24.802452Z",
     "shell.execute_reply": "2021-04-28T22:37:24.797457Z"
    },
    "id": "designing-dividend",
    "papermill": {
     "duration": 116.33689,
     "end_time": "2021-04-28T22:37:24.803401",
     "exception": false,
     "start_time": "2021-04-28T22:35:28.466511",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pandas-profiling 3.2.0 requires matplotlib>=3.2.0, but you have matplotlib 3.1.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU pinecone-client imgaug==0.2.5\n",
    "!pip install -q torchvision pandas matplotlib==3.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-presence",
   "metadata": {
    "id": "prospective-presence",
    "papermill": {
     "duration": 0.047351,
     "end_time": "2021-04-28T22:37:24.905190",
     "exception": false,
     "start_time": "2021-04-28T22:37:24.857839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Set up Pinecone. Get your Pinecone API key [here](https://www.pinecone.io/start/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "featured-boston",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-28T22:37:25.000364Z",
     "iopub.status.busy": "2021-04-28T22:37:24.999568Z",
     "iopub.status.idle": "2021-04-28T22:37:25.954645Z",
     "shell.execute_reply": "2021-04-28T22:37:25.953711Z"
    },
    "id": "featured-boston",
    "papermill": {
     "duration": 1.00482,
     "end_time": "2021-04-28T22:37:25.954847",
     "exception": false,
     "start_time": "2021-04-28T22:37:24.950027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pinecone\n",
    "import os\n",
    "\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\") or \"YOUR_API_KEY\"\n",
    "pinecone.init(api_key=api_key, environment='us-west1-gcp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-colony",
   "metadata": {
    "id": "sitting-colony",
    "papermill": {
     "duration": 0.046053,
     "end_time": "2021-04-28T22:37:26.485100",
     "exception": false,
     "start_time": "2021-04-28T22:37:26.439047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-weekend",
   "metadata": {
    "id": "hairy-weekend",
    "papermill": {
     "duration": 0.043853,
     "end_time": "2021-04-28T22:37:26.576234",
     "exception": false,
     "start_time": "2021-04-28T22:37:26.532381",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this example, we will use the Tiny ImageNet dataset http://cs231n.stanford.edu/, which has 100,000 training images cross 200 classes. Each class has 500 training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-trash",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-28T22:37:26.675403Z",
     "iopub.status.busy": "2021-04-28T22:37:26.672333Z",
     "iopub.status.idle": "2021-04-28T22:38:03.218311Z",
     "shell.execute_reply": "2021-04-28T22:38:03.217681Z"
    },
    "id": "rapid-trash",
    "papermill": {
     "duration": 36.598787,
     "end_time": "2021-04-28T22:38:03.218561",
     "exception": false,
     "start_time": "2021-04-28T22:37:26.619774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests, os, zipfile\n",
    "\n",
    "DATA_DIR = \"tmp\"\n",
    "IMAGENET_DIR = f\"{DATA_DIR}/tiny-imagenet-200\"\n",
    "IMAGENET_ZIP = f\"{DATA_DIR}/tiny-imagenet-200.zip\"\n",
    "IMAGENET_URL = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
    "\n",
    "\n",
    "def download_data():\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(IMAGENET_DIR):\n",
    "        if not os.path.exists(IMAGENET_ZIP):\n",
    "            r = requests.get(IMAGENET_URL)  # create HTTP response object\n",
    "            with open(IMAGENET_ZIP, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "\n",
    "        with zipfile.ZipFile(IMAGENET_ZIP, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(DATA_DIR)\n",
    "\n",
    "\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-rally",
   "metadata": {
    "id": "brazilian-rally",
    "papermill": {
     "duration": 0.048493,
     "end_time": "2021-04-28T22:38:03.316994",
     "exception": false,
     "start_time": "2021-04-28T22:38:03.268501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "`torchvision` is a popular library from PyTorch(`torch`) that provides convenient functions for loading and transforming images. Here we use `torchvision` to select images that belong to a random subset of image classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-citation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-28T22:38:03.411794Z",
     "iopub.status.busy": "2021-04-28T22:38:03.411127Z",
     "iopub.status.idle": "2021-04-28T22:38:04.708449Z",
     "shell.execute_reply": "2021-04-28T22:38:04.707575Z"
    },
    "id": "thousand-citation",
    "papermill": {
     "duration": 1.347351,
     "end_time": "2021-04-28T22:38:04.708670",
     "exception": false,
     "start_time": "2021-04-28T22:38:03.361319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import random\n",
    "\n",
    "random_seed = 123\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Select a random sample of image classes\n",
    "image_classes = set(random.sample(range(200), 5))\n",
    "\n",
    "# Get the corresponding image file names\n",
    "image_file_names = [\n",
    "    file_name\n",
    "    for file_name, label in datasets.ImageFolder(f\"{IMAGENET_DIR}/train\").imgs\n",
    "    if label in image_classes\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-blanket",
   "metadata": {
    "id": "mathematical-blanket",
    "papermill": {
     "duration": 0.047173,
     "end_time": "2021-04-28T22:38:04.800569",
     "exception": false,
     "start_time": "2021-04-28T22:38:04.753396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's look at some of selected images. We'll write two utility functions to show the images, which will also be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-parish",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-28T22:38:04.899892Z",
     "iopub.status.busy": "2021-04-28T22:38:04.899186Z",
     "iopub.status.idle": "2021-04-28T22:38:05.437744Z",
     "shell.execute_reply": "2021-04-28T22:38:05.437043Z"
    },
    "id": "thorough-parish",
    "papermill": {
     "duration": 0.591671,
     "end_time": "2021-04-28T22:38:05.438045",
     "exception": false,
     "start_time": "2021-04-28T22:38:04.846374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def show_images_horizontally(file_names):\n",
    "    m = len(file_names)\n",
    "    fig, ax = plt.subplots(1, m)\n",
    "    fig.set_figwidth(1.5 * m)\n",
    "    for a, f in zip(ax, file_names):\n",
    "        a.imshow(Image.open(f))\n",
    "        a.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_image(file_name):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    fig.set_figwidth(1.3)\n",
    "    ax.imshow(Image.open(file_name))\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-jaguar",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "execution": {
     "iopub.execute_input": "2021-04-28T22:38:05.534987Z",
     "iopub.status.busy": "2021-04-28T22:38:05.533843Z",
     "iopub.status.idle": "2021-04-28T22:38:07.238221Z",
     "shell.execute_reply": "2021-04-28T22:38:07.238665Z"
    },
    "id": "integrated-jaguar",
    "outputId": "ee98ec9a-5e8f-45fa-fad2-7784a87875f6",
    "papermill": {
     "duration": 1.756205,
     "end_time": "2021-04-28T22:38:07.238941",
     "exception": false,
     "start_time": "2021-04-28T22:38:05.482736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    show_images_horizontally(random.sample(image_file_names, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-sphere",
   "metadata": {
    "id": "focused-sphere",
    "papermill": {
     "duration": 0.063259,
     "end_time": "2021-04-28T22:38:07.362568",
     "exception": false,
     "start_time": "2021-04-28T22:38:07.299309",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Convert images to embeddings using a pre-trained neural network model\n",
    "\n",
    "[Vector embeddings](https://www.pinecone.io/learn/what-are-vectors-embeddings/) for images are representations of images as high dimensional vectors. \n",
    "\n",
    "A typical image file consists of values of pixel intensities in three spectra: red, green, and blue. A naïve way of converting an image file to a vector is by \"flattening\" the image file--that is, arranging all pixel value in one row. Sadly this is practically useless for most machine learning applications.\n",
    "\n",
    "A much better way of creating a vector representation of an image is to apply a computer vision (CV) model to the image. The output of the CV model is a vector embedding of the image optimized for assessing the image similarity. In other words, the CV model maps similar images to vectors that are close to each other, and dissimilar images to vectors that are far apart. By converting images to vector embeddings, we can simplify the problem of image similarity search to that of vector proximity search.\n",
    "\n",
    "In practice, we don't always have to train a new CV model. In this example, we will use a pre-trained model from `torchvision` called `squeezenet` to convert the images. Feel free to experiment with other models provided by `torchvision` to see which model best fits your application. See `torchvision` [examples page](https://pytorch.org/vision/0.8/models.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-montana",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-04-28T22:38:07.491973Z",
     "iopub.status.busy": "2021-04-28T22:38:07.491263Z",
     "iopub.status.idle": "2021-04-28T22:38:08.529058Z",
     "shell.execute_reply": "2021-04-28T22:38:08.529475Z"
    },
    "id": "champion-montana",
    "outputId": "702347a0-fee2-4dc9-86d9-1324e3c1dfef",
    "papermill": {
     "duration": 1.107876,
     "end_time": "2021-04-28T22:38:08.529699",
     "exception": false,
     "start_time": "2021-04-28T22:38:07.421823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms as ts\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class ImageEmbedder:\n",
    "    def __init__(self):\n",
    "        self.normalize = ts.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "        # see sss0.8/models.html for many more model options\n",
    "        self.model = models.squeezenet1_0(pretrained=True, progress=False)  # squeezenet\n",
    "\n",
    "    def embed(self, image_file_name):\n",
    "        image = Image.open(image_file_name).convert(\"RGB\")\n",
    "        image = ts.Resize(256)(image)\n",
    "        image = ts.CenterCrop(224)(image)\n",
    "        tensor = ts.ToTensor()(image)\n",
    "        tensor = self.normalize(tensor).reshape(1, 3, 224, 224)\n",
    "        vector = self.model(tensor).cpu().detach().numpy().flatten()\n",
    "        return vector\n",
    "\n",
    "\n",
    "image_embedder = ImageEmbedder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-humor",
   "metadata": {
    "id": "received-humor",
    "papermill": {
     "duration": 0.064954,
     "end_time": "2021-04-28T22:38:08.667035",
     "exception": false,
     "start_time": "2021-04-28T22:38:08.602081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Once we have the image embedder class, we can use it to convert image files to vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-picking",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-04-28T22:38:08.801206Z",
     "iopub.status.busy": "2021-04-28T22:38:08.800543Z",
     "iopub.status.idle": "2021-04-28T22:39:24.056555Z",
     "shell.execute_reply": "2021-04-28T22:39:24.055918Z"
    },
    "id": "offensive-picking",
    "outputId": "fa03d66d-7b8c-4ae7-b43d-df7833b4f226",
    "papermill": {
     "duration": 75.323834,
     "end_time": "2021-04-28T22:39:24.056783",
     "exception": false,
     "start_time": "2021-04-28T22:38:08.732949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"image_file_name\"] = image_file_names\n",
    "df[\"embedding_id\"] = [\n",
    "    file_name.split(IMAGENET_DIR)[-1] for file_name in image_file_names\n",
    "]\n",
    "df[\"embedding\"] = [\n",
    "    image_embedder.embed(file_name).tolist()\n",
    "    for file_name in tqdm(image_file_names, miniters=int(len(image_file_names) / 100))\n",
    "]\n",
    "df = df.sample(frac=1)  # shuffle the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-defense",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-04-28T22:39:24.253144Z",
     "iopub.status.busy": "2021-04-28T22:39:24.252507Z",
     "iopub.status.idle": "2021-04-28T22:39:24.255804Z",
     "shell.execute_reply": "2021-04-28T22:39:24.255245Z"
    },
    "id": "precious-defense",
    "outputId": "e605b728-93d7-4bf6-8004-994bff96c067",
    "papermill": {
     "duration": 0.10825,
     "end_time": "2021-04-28T22:39:24.256008",
     "exception": false,
     "start_time": "2021-04-28T22:39:24.147758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-diploma",
   "metadata": {
    "id": "apparent-diploma",
    "papermill": {
     "duration": 0.096318,
     "end_time": "2021-04-28T22:39:24.445249",
     "exception": false,
     "start_time": "2021-04-28T22:39:24.348931",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Split the data into the items dataset and the queries dataset. Because the data is already shuffled, we can directly split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-strand",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-04-28T22:39:24.638769Z",
     "iopub.status.busy": "2021-04-28T22:39:24.638059Z",
     "iopub.status.idle": "2021-04-28T22:39:24.642142Z",
     "shell.execute_reply": "2021-04-28T22:39:24.641577Z"
    },
    "id": "false-strand",
    "outputId": "d4b768b3-d69b-43a6-eec9-58d6b1b388a1",
    "papermill": {
     "duration": 0.104335,
     "end_time": "2021-04-28T22:39:24.642327",
     "exception": false,
     "start_time": "2021-04-28T22:39:24.537992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cutoff = int(len(df) * 0.95)\n",
    "item_df, query_df = df[:cutoff], df[cutoff:]\n",
    "(item_df.shape, query_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-crowd",
   "metadata": {
    "id": "muslim-crowd",
    "papermill": {
     "duration": 0.096645,
     "end_time": "2021-04-28T22:39:24.833896",
     "exception": false,
     "start_time": "2021-04-28T22:39:24.737251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create a Pinecone vector index service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-fiber",
   "metadata": {
    "id": "forbidden-fiber",
    "papermill": {
     "duration": 0.095706,
     "end_time": "2021-04-28T22:39:25.023202",
     "exception": false,
     "start_time": "2021-04-28T22:39:24.927496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that we have our vector embeddings we can create our Pinecone service and upload the data to it.\n",
    "\n",
    "Below we create a Pinecone index. Here we use two arguments: \n",
    "- `metric=\"euclidean\"` means the vector search will use euclidean distance as the measure of similarity.\n",
    "- `shards=1` means the service will run on a single node. Since we only index at most 100k vectors in 1000 dimensions, a single shard is more than enough. If you have more than 1M vectors, you should increase the number of shards. Refer to the [documentation](https://docs.beta.pinecone.io/en/latest/python_client/graph.html#pinecone.graph.IndexGraph) for guidelines on how to choose the number of shards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-newspaper",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-28T22:39:25.220573Z",
     "iopub.status.busy": "2021-04-28T22:39:25.219880Z",
     "iopub.status.idle": "2021-04-28T22:39:39.429723Z",
     "shell.execute_reply": "2021-04-28T22:39:39.428972Z"
    },
    "id": "secondary-newspaper",
    "papermill": {
     "duration": 14.312249,
     "end_time": "2021-04-28T22:39:39.429989",
     "exception": false,
     "start_time": "2021-04-28T22:39:25.117740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choosing an arbitrary name for my index\n",
    "index_name = \"simple-pytorch-image-search\"\n",
    "\n",
    "# Checking whether the index already exists.\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(index_name, dimension=1000, metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-satellite",
   "metadata": {
    "id": "supported-satellite",
    "papermill": {
     "duration": 0.097401,
     "end_time": "2021-04-28T22:39:39.630194",
     "exception": false,
     "start_time": "2021-04-28T22:39:39.532793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that the index is created, we can upload vectorized images. To do that, we connect to the index. You only need to do this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-electron",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-28T22:39:39.961566Z",
     "iopub.status.busy": "2021-04-28T22:39:39.960845Z",
     "iopub.status.idle": "2021-04-28T22:39:40.363424Z",
     "shell.execute_reply": "2021-04-28T22:39:40.362587Z"
    },
    "id": "lasting-electron",
    "papermill": {
     "duration": 0.63738,
     "end_time": "2021-04-28T22:39:40.363690",
     "exception": false,
     "start_time": "2021-04-28T22:39:39.726310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "index = pinecone.Index(index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-aspect",
   "metadata": {
    "id": "checked-aspect",
    "papermill": {
     "duration": 0.096842,
     "end_time": "2021-04-28T22:39:40.566706",
     "exception": false,
     "start_time": "2021-04-28T22:39:40.469864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "`upsert` will upload, add, and index items into your remote vector similarity search service. If an item with that ID already exists it will be overwritten with the new provided value. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e70c9b",
   "metadata": {
    "id": "12e70c9b"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def chunks(iterable, batch_size=100):\n",
    "    it = iter(iterable)\n",
    "    chunk = tuple(itertools.islice(it, batch_size))\n",
    "    while chunk:\n",
    "        yield chunk\n",
    "        chunk = tuple(itertools.islice(it, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-basic",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-28T22:39:40.766523Z",
     "iopub.status.busy": "2021-04-28T22:39:40.765845Z",
     "iopub.status.idle": "2021-04-28T22:39:54.503344Z",
     "shell.execute_reply": "2021-04-28T22:39:54.502526Z"
    },
    "id": "cellular-basic",
    "papermill": {
     "duration": 13.839044,
     "end_time": "2021-04-28T22:39:54.503606",
     "exception": false,
     "start_time": "2021-04-28T22:39:40.664562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch in chunks(zip(item_df.embedding_id, item_df.embedding), 50):\n",
    "    index.upsert(vectors=batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-driving",
   "metadata": {
    "id": "behind-driving",
    "papermill": {
     "duration": 0.099945,
     "end_time": "2021-04-28T22:39:54.703643",
     "exception": false,
     "start_time": "2021-04-28T22:39:54.603698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Querying "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-notice",
   "metadata": {
    "id": "lonely-notice",
    "papermill": {
     "duration": 0.103076,
     "end_time": "2021-04-28T22:39:54.902705",
     "exception": false,
     "start_time": "2021-04-28T22:39:54.799629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's test dataset as test queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-sewing",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-04-28T22:39:55.520146Z",
     "iopub.status.busy": "2021-04-28T22:39:55.519482Z",
     "iopub.status.idle": "2021-04-28T22:39:56.354032Z",
     "shell.execute_reply": "2021-04-28T22:39:56.354632Z"
    },
    "id": "cubic-sewing",
    "outputId": "114190d7-af8f-4f35-a682-2aec993b498a",
    "papermill": {
     "duration": 0.94281,
     "end_time": "2021-04-28T22:39:56.354847",
     "exception": false,
     "start_time": "2021-04-28T22:39:55.412037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "start = time.perf_counter()\n",
    "total_res = list()\n",
    "for batch in chunks(query_df.embedding,10):\n",
    "    res = index.query(batch, top_k=10)  # issuing queries\n",
    "    total_res += [res.matches for res in res.results]\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(\"Run this test on a fast network to get the best performance.\")\n",
    "# print(f\"Service throughput is {int(len(query_df)/(end-start))} queries/second.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d71d35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01d71d35",
    "outputId": "13076612-5453-4c29-961f-edb7de96df03"
   },
   "outputs": [],
   "source": [
    "len(total_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-swing",
   "metadata": {
    "id": "vanilla-swing",
    "papermill": {
     "duration": 0.102206,
     "end_time": "2021-04-28T22:40:02.831196",
     "exception": false,
     "start_time": "2021-04-28T22:40:02.728990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that we have our results, we can look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-entry",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2021-04-28T22:40:03.060067Z",
     "iopub.status.busy": "2021-04-28T22:40:03.059377Z",
     "iopub.status.idle": "2021-04-28T22:40:06.859380Z",
     "shell.execute_reply": "2021-04-28T22:40:06.858714Z"
    },
    "id": "smaller-entry",
    "outputId": "a1a42224-4610-4d62-9d23-c3cab0ff639b",
    "papermill": {
     "duration": 3.916445,
     "end_time": "2021-04-28T22:40:06.859582",
     "exception": false,
     "start_time": "2021-04-28T22:40:02.943137",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(30, 40):\n",
    "    print(f\"Query {i+1} and search results\")\n",
    "    ids = [match.id for match in total_res[i]]\n",
    "    show_image(query_df.image_file_name.iloc[i])\n",
    "    show_images_horizontally(\n",
    "        [IMAGENET_DIR + embedding_id for embedding_id in ids]\n",
    "    )\n",
    "\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-pennsylvania",
   "metadata": {
    "id": "sealed-pennsylvania",
    "papermill": {
     "duration": 0.144059,
     "end_time": "2021-04-28T22:40:07.144120",
     "exception": false,
     "start_time": "2021-04-28T22:40:07.000061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As is shown above, image similarity search works relatively well with the pre-trained CV model: when we query with images that are not in the index, most similarity seaches return images from the same class as the query images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-sampling",
   "metadata": {
    "id": "nasty-sampling",
    "papermill": {
     "duration": 0.143404,
     "end_time": "2021-04-28T22:40:07.435171",
     "exception": false,
     "start_time": "2021-04-28T22:40:07.291767",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Wrap up\n",
    "\n",
    "You can use `pinecone.delete_index` to delete the index and free all resources dedicated to it. Once an index is deleted all resources need to be recreated. We suggest that you only delete an index if no application is using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-luther",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-28T22:40:07.732453Z",
     "iopub.status.busy": "2021-04-28T22:40:07.731773Z",
     "iopub.status.idle": "2021-04-28T22:40:21.250040Z",
     "shell.execute_reply": "2021-04-28T22:40:21.249223Z"
    },
    "id": "operating-luther",
    "papermill": {
     "duration": 13.66957,
     "end_time": "2021-04-28T22:40:21.250291",
     "exception": false,
     "start_time": "2021-04-28T22:40:07.580721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pinecone.delete_index(index_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "simple_pytorch_image_search.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 295.210786,
   "end_time": "2021-04-28T22:40:22.227918",
   "environment_variables": {},
   "exception": null,
   "input_path": "/notebooks/image_search/simple_pytorch_image_search.ipynb",
   "output_path": "/notebooks/tmp/image_search/simple_pytorch_image_search.ipynb",
   "parameters": {},
   "start_time": "2021-04-28T22:35:27.017132",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
